{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import json\n",
    "import numpy as np\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from src.detector.utils import binary_mask_iou, mask_convert\n",
    "from src.tracker.data_track_precomputed import MOT16SequencesPrecomputed\n",
    "from src.tracker.data_track import MOT16Sequences\n",
    "from src.utils.torch_utils import dict2keys_and_items\n",
    "from src.detector.visualize import visualize_detection\n",
    "\n",
    "from src.detector.object_detector import init_detector\n",
    "\n",
    "\n",
    "sequences = MOT16Sequences(\n",
    "        root_dir=\"data/MOT16\",\n",
    "        dataset=\"MOT16-train\",\n",
    "        vis_threshold=0.5,\n",
    ")\n",
    "sequence = sequences[0]\n",
    "frame = sequence[10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arthu\\anaconda3\\envs\\DL_env\\lib\\site-packages\\torch\\functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ..\\aten\\src\\ATen\\native\\TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "config_path = \"config/obj_detect/coco_maskrcnn_experiment.json\"\n",
    "\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = json.load(f)\n",
    "obj_detect = init_detector(**config)\n",
    "obj_detect.eval();\n",
    "with torch.no_grad():\n",
    "    det = obj_detect([frame[\"img\"]])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from filterpy.kalman import KalmanFilter\n",
    "kf = KalmanFilter(dim_x=7, dim_z=4) \n",
    "kf.F = np.array([[1,0,0,0,1,0,0],[0,1,0,0,0,1,0],[0,0,1,0,0,0,1],[0,0,0,1,0,0,0],  [0,0,0,0,1,0,0],[0,0,0,0,0,1,0],[0,0,0,0,0,0,1]])\n",
    "kf.H = np.array([[1,0,0,0,0,0,0],[0,1,0,0,0,0,0],[0,0,1,0,0,0,0],[0,0,0,1,0,0,0]])\n",
    "kf.Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2 \n",
    "cv_filt = cv2.KalmanFilter(7, 4)\n",
    "cv_filt.errorCovPre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1     2     3\n",
       "0  0.47  0.47  0.47  0.47\n",
       "1  0.47  0.47  0.47  0.47\n",
       "2  0.47  0.47  0.47  0.47\n",
       "3  0.47  0.47  0.47  0.47"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.DataFrame(data=0.46584*np.ones((4,4)))\n",
    "df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.detector.utils import keypoint_convert\n",
    "from src.detector.object_detector import body_part_combination\n",
    "def correct_box_sizes_using_keypoints(\n",
    "    detection_batch,\n",
    "    keypoint_score_thresh=0.9,\n",
    "    scale_factor_width=1.15,\n",
    "    scale_factor_height=1.15,\n",
    "):\n",
    "    for det in detection_batch:\n",
    "        occlusion_masks = get_occlusion_inside_obj_boxes(boxes=det[\"boxes\"], masks=det[\"masks\"])\n",
    "        occlusion_direction = get_occlusion_direction(occlusion_masks)\n",
    "\n",
    "        le_idxs = keypoint_convert(\n",
    "            body_part_combination[\"left_extremity\"], \"name\", \"idx\"\n",
    "        )\n",
    "        re_idxs = keypoint_convert(\n",
    "            body_part_combination[\"right_extremity\"], \"name\", \"idx\"\n",
    "        )\n",
    "        f_idxs = keypoint_convert(body_part_combination[\"feet\"], \"name\", \"idx\")\n",
    "        xywh = torchvision.ops.box_convert(det[\"boxes\"].clone(), \"xyxy\", \"xywh\")\n",
    "\n",
    "        for obj_idx in range(len(det[\"keypoints_scores\"])):\n",
    "            found_keypoint_idxs = torch.where(\n",
    "                det[\"keypoints_scores\"][obj_idx].sigmoid()\n",
    "                > keypoint_score_thresh\n",
    "            )[0].tolist()\n",
    "            if set(f_idxs) == (set(f_idxs) - set(found_keypoint_idxs)) and occlusion_direction[\"bottom\"][obj_idx]:\n",
    "                xywh[obj_idx, 3] *= scale_factor_height\n",
    "\n",
    "            if len((set(re_idxs) - set(found_keypoint_idxs))) > 0 and occlusion_direction[\"left\"][obj_idx]:\n",
    "                xywh[obj_idx, 0] -= (scale_factor_width - 1) * xywh[obj_idx, 2]\n",
    "                xywh[obj_idx, 2] *= scale_factor_width\n",
    "\n",
    "            if len((set(le_idxs) - set(found_keypoint_idxs))) > 0 and occlusion_direction[\"right\"][obj_idx]:\n",
    "                xywh[obj_idx, 2] *= scale_factor_width\n",
    "        boxes = torchvision.ops.box_convert(xywh, \"xywh\", \"xyxy\")\n",
    "        det[\"boxes\"] = boxes\n",
    "    return detection_batch\n",
    "\n",
    "def get_occlusion_inside_obj_boxes(boxes, masks, output_size=(4, 4)):\n",
    "    iou = torchvision.ops.box_iou(boxes, boxes)\n",
    "    overlap_pools = []\n",
    "    for obj_id in range(len(boxes)):\n",
    "        neighbours = torch.logical_and(iou[obj_id] > 0, iou[obj_id] < 1)\n",
    "        x,y,w,h = torchvision.ops.box_convert(boxes[[obj_id]], \"xyxy\", \"xywh\").squeeze().int()\n",
    "        mask_crops = TF.crop(masks[neighbours], top=y, left=x, height=h, width=w)\n",
    "        overlap = (mask_crops > 0).any(dim=0).float()\n",
    "        overlap_pool = TF.resize(overlap, size=output_size)\n",
    "        overlap_pools.append(overlap_pool)\n",
    "    overlap_pools = torch.stack(overlap_pools, dim=0).squeeze(1)\n",
    "    return overlap_pools\n",
    "\n",
    "def get_occlusion_direction(occlusion_masks):\n",
    "    return {\n",
    "        \"left\": occlusion_masks[:, 1:3, 0:1].any(dim=1),\n",
    "        \"right\": occlusion_masks[:, 1:3, 3:4].any(dim=1),\n",
    "        \"bottom\": occlusion_masks[:, 3:4, 1:3].any(dim=2),\n",
    "        \"top\": occlusion_masks[:, 0:1, 1:3].any(dim=2)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.detector.object_detector import body_part_combination\n",
    "from src.detector.utils import keypoint_convert, mask_io_min_max, mask_area_nms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : MANUALLY\n",
    "# for all boxes:\n",
    "# if left or right wrist/elbow/shoulder/fuß (probiere any/all) not visible -> widen box by XXX percent in that direction\n",
    "# if one foot not visible -> widen box by XXX percent in bottom direction\n",
    "\n",
    "\n",
    "# for low score boxes:\n",
    "# if left boy parts are expected to be visible (based on mask iou or box iou) but are not, and the object has low score, remove it\n",
    "\n",
    "\n",
    "# TODO : EXTEND THIS TO A MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_box_correction_features_from_det(det):\n",
    "    \"\"\"\n",
    "    for every detection get features for the box corrector\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    det: Dict[str: Tensor[N, ...]]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    features: [N, num_features]\n",
    "    \"\"\"\n",
    "    boxes = det[\"boxes\"].int()\n",
    "    masks = det[\"masks\"]\n",
    "    scores = det[\"scores\"]\n",
    "\n",
    "    overlap_pools = get_occlusion_inside_obj_boxes(boxes, masks)\n",
    "    overlap_features = overlap_pools.reshape(len(boxes), -1)\n",
    "\n",
    "    # keypoint score are good predictor for visibility of keypoint\n",
    "    keypoint_score_features = det[\"keypoints_scores\"]\n",
    "\n",
    "    # confidence features: if you want object classification then add these\n",
    "    # score\n",
    "    # entropy\n",
    "\n",
    "    # keep this because all boxes are resized to same size\n",
    "    areas = torchvision.ops.box_area(boxes).unsqueeze(1)\n",
    "    _, _, w, h = torchvision.ops.box_convert(boxes, \"xyxy\", \"xywh\").T\n",
    "    ratios = (h/w).unsqueeze(1)\n",
    "    box_features = torch.cat([areas, ratios], dim=1)\n",
    "\n",
    "    # relative positions of certain keypoints, we use head (=both ears)\n",
    "    keypoint_names = ['left_ear', 'right_ear']\n",
    "    keypoint_idxs = keypoint_convert(keypoint_names, \"name\", \"idx\")\n",
    "    keypoint_pos = det[\"keypoints\"][:, keypoint_idxs, :2] \n",
    "    box_min = boxes[:, [0, 1]]\n",
    "    box_max = boxes[:, [2, 3]]\n",
    "    rel_keypoint_pos = ((keypoint_pos.permute(1, 0, 2) - box_min) / (box_max - box_min)).permute(1, 0, 2)\n",
    "    keypoint_pos_features = rel_keypoint_pos.reshape(len(boxes), -1)\n",
    "\n",
    "    # head position might be useful, because head it says something about box height\n",
    "    features = torch.cat([overlap_features, keypoint_score_features, keypoint_pos_features, box_features], dim=1)\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : setze keypoint depection ein um False Positives zu vermeiden\n",
    "# Sichere Verbesserung, low risk: wenn objekt keinen overlap mit anderen objekten hat und ausreichende größe, muss es mindestens 90% der Keypoints haben\n",
    "# High Potential, High Risk: analysiere bei welchem objekt welche keypoints sichtbar sind und überleg dir was ...\n",
    "# z.B. NMS mit köpfen, wenn zwei boxen overlappen, dann schau ob köpfe overlappen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : ich habe schon ausgetestet wie viele misses wir aufgrund nms haben, aber noch nicht, wie viele misses wir aufgrund core_thresh haben.\n",
    "# probiere mal nms 100 mit score_thresh: 0 und schau wie viele low score detections wir durch movement / keypoints o.ä. erkennen können. Undzwar ohne Byte. Also aus dem inherenten zustand."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "baed9b6b162aa45400c1f6f8e8b953cc621cc8f25bcb3799f89f5a2556aea4e5"
  },
  "kernelspec": {
   "display_name": "Python (DL_env)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
